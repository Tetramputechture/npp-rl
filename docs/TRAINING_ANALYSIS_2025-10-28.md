# Comprehensive RL Training Analysis - October 28, 2025

## Executive Summary

This document provides a comprehensive analysis of the N++ RL training setup based on TensorBoard logs, configuration files, and codebase review. The analysis reveals **critical issues preventing effective learning**, with the agent stuck at curriculum stage 2 (simple) with only 14% success rate after 1M timesteps.

**Key Finding:** The agent is learning, but converging to a suboptimal policy due to several compounding issues: disabled PBRS (no dense rewards), low entropy coefficient (premature convergence), overly aggressive curriculum, and training duration too short for the problem complexity.

---

## 1. Data Sources Analyzed

### TensorBoard Metrics
- **File:** `training-results/events.out.tfevents.1761619710.129-146-49-69.17882.1`
- **Training Duration:** 1,010,688 timesteps (47 rollout steps)
- **Data Points:**
  - Curriculum metrics: 47 samples
  - Episode metrics: 9,621 samples  
  - Training metrics: 46 samples
  - Gradient norms: 1,002 samples
  - Action distributions: 9,625 samples

### Configuration
- **File:** `training-results/config.json`
- **Experiment:** mlp-baseline-1027-v2
- **Key Settings:**
  - Architecture: mlp_baseline
  - Environments: 21 parallel
  - Timesteps: 1,000,000
  - BC Pretraining: 50 epochs
  - Curriculum: Enabled (threshold 0.7, min 100 episodes)
  - **PBRS: DISABLED** âš ï¸

### Code Review
- PPO hyperparameters (`ppo_hyperparameters.py`)
- Reward constants (`reward_constants.py`)
- Feature extractors (`configurable_extractor.py`)
- Architecture configs (`architecture_configs.py`)
- Simulation mechanics (`sim_mechanics_doc.md`)

---

## 2. Critical Issues Identified

### ðŸ”´ CRITICAL #1: Curriculum Learning Failure

**Observation:**
- Agent stuck on Stage 2 ("simple") for entire training run
- Success rate: 14% (well below 70% threshold)
- Never progressed to stages 3-6 (medium, exploration, mine_heavy, complex)
- Success rate **declined** from peak of 33.3% (step 301k) to 14% (step 1M)

**Curriculum Stage Performance:**
```
Stage 0 (simplest):     65 episodes, 100.0% success rate âœ“
Stage 1 (simpler):      71 episodes,  64.0% success rate âœ“
Stage 2 (simple):       65 episodes,  14.0% success rate âœ— STUCK
Stage 3 (medium):        0 episodes,   0.0% success rate (never reached)
Stage 4 (exploration):   0 episodes,   0.0% success rate (never reached)
Stage 5 (mine_heavy):    0 episodes,   0.0% success rate (never reached)
Stage 6 (complex):       0 episodes,   0.0% success rate (never reached)
```

**Root Causes:**
1. **Threshold too high:** 70% success rate is aggressive for platformers
2. **Sparse rewards:** Without PBRS, agent gets minimal feedback
3. **Training too short:** 1M steps insufficient for complex exploration
4. **No adaptive progression:** Fixed threshold ignores improvement trends

**Impact:** Agent cannot access higher complexity levels, limiting generalization capability.

---

### ðŸ”´ CRITICAL #2: PBRS Disabled (No Dense Rewards)

**Observation:**
```python
# From config.json
"enable_pbrs": false
```

```
PBRS Rewards (Mean):
  Navigation:  -0.000000
  Exploration:  0.000000  
  PBRS:        -0.004419
  Total:       -0.004519
```

**Consequence:** Agent only receives sparse terminal rewards:
- Level completion: +10.0 (rare)
- Switch activation: +1.0 (occasional)
- Death: -0.5 (frequent)
- Time penalty: -0.0001 per step (constant)

**Why This Is Critical:**
- Potential-Based Reward Shaping (PBRS) provides **policy-invariant** dense rewards
- Without PBRS, agent must explore randomly until stumbling upon objectives
- In complex levels, random exploration is exponentially unlikely to succeed
- This creates a "credit assignment" problem - agent doesn't know which actions led to success

**Expected Behavior with PBRS:**
- Navigation rewards guide agent toward switch/exit
- Exploration rewards encourage spatial coverage
- Dense signal enables gradient-based learning

**Actual Behavior without PBRS:**
- Agent wanders aimlessly
- No feedback until death or (rarely) completion
- Learning signal is too sparse for complex navigation

---

### ðŸ”´ CRITICAL #3: Action Space Collapse (Jump Avoidance)

**Observation:**
```
Action Frequency Changes:
  NOOP         : 0.152 -> 0.155 (+0.003)
  Left         : 0.143 -> 0.223 (+0.080)  â¬†ï¸ +56%
  Right        : 0.171 -> 0.258 (+0.087)  â¬†ï¸ +51%
  Jump         : 0.133 -> 0.118 (-0.016)  â¬‡ï¸ -12%
  Jump+Left    : 0.229 -> 0.160 (-0.068)  â¬‡ï¸ -30%
  Jump+Right   : 0.171 -> 0.085 (-0.086)  â¬‡ï¸ -50%
```

**Why This Is Catastrophic:**
- N++ is a **platformer** - jumping is ESSENTIAL for level completion
- Agent learned to walk instead of jump (safer but insufficient)
- Combined jump actions (Jump+Left, Jump+Right) are critical for platforming
- This indicates agent learned a "locally safe" but globally suboptimal policy

**Root Causes:**
1. **Sparse rewards favor "safe" actions:** Walking produces fewer deaths
2. **No reward for risky exploration:** Jumping over gaps has high variance
3. **Low entropy coefficient:** Policy converged prematurely to safe actions
4. **Death penalty without progress reward:** Agent avoids risk without incentive to explore

**Impact:** Agent physically cannot complete levels requiring jumps (which is most levels).

---

### ðŸ”´ CRITICAL #4: Low Entropy Coefficient (Premature Convergence)

**Observation:**
```python
# From ppo_hyperparameters.py
"ent_coef": 0.002720504247658009  # âš ï¸ VERY LOW
```

```
Action Entropy:
  Initial: 1.7753
  Final:   1.7289
  Maximum (uniform): log(6) = 1.7918
```

**Analysis:**
- Entropy dropped by 2.6% over training
- Current entropy: 96.5% of maximum (policy becoming deterministic)
- For comparison, typical RL training uses ent_coef = 0.01 to 0.1
- **Current value is 4-40x lower than recommended**

**Consequence:**
- Policy converged to deterministic behavior before finding good solution
- Exploration effectively shut down
- Agent locked into suboptimal "safe walking" policy
- Cannot discover jump-based solutions that require exploration

**Expected Behavior:**
- Entropy should decrease gradually over millions of timesteps
- Higher ent_coef maintains exploration longer
- Allows agent to discover non-obvious solutions (jumping mechanics)

---

### ðŸ”´ CRITICAL #5: Negative Value Estimates Indicate Pessimism

**Observation:**
```
Value Function Estimates:
  Initial Mean: -0.5852
  Final Mean:   -2.7555  â¬‡ï¸ Increasingly pessimistic
  Std Dev:       0.1734  (low variance, high confidence)
```

**Interpretation:**
- Agent learned that episodes lead to negative outcomes
- This is **accurate** given current policy (14% success rate)
- Negative values reflect: deaths (-0.5) + time penalties accumulating
- Low variance indicates value function is confident in this pessimistic assessment

**Why This Matters:**
- Value estimates drive policy improvement via advantage function
- Pessimistic values mean agent sees little benefit to exploration
- Creates self-reinforcing cycle: pessimism â†’ risk aversion â†’ failure â†’ more pessimism
- Explained variance of 52.9% shows value function is learning, but learning to predict negative returns

---

### ðŸŸ¡ MODERATE #6: Training Duration Insufficient

**Observation:**
- Total timesteps: 1,000,000
- Parallel envs: 21
- Timesteps per env: ~47,619
- Episodes completed: ~201 total

**Benchmark Comparison:**
- Atari games (PPO): 10M-50M timesteps typical
- Complex platformers: 20M-100M timesteps
- Curriculum learning: Often requires 5-10M per stage

**Current Allocation:**
- Stage 0 (simplest): 65 episodes
- Stage 1 (simpler): 71 episodes  
- Stage 2 (simple): 65 episodes
- **Average ~67 episodes per stage before stalling**

**Impact:**
- Insufficient exploration time for complex environments
- Curriculum can't progress naturally
- Agent doesn't experience enough diversity in simpler stages
- No time for fine-tuning after initial learning

---

### ðŸŸ¡ MODERATE #7: Gradient Clipping May Be Too Aggressive

**Observation:**
```
Gradient Norms:
  Initial:  0.0000
  Final:    0.5000
  Mean:     0.4891
  Max:      0.5000  âš ï¸ Consistently at max
  Min:      0.0000
```

```python
# From ppo_hyperparameters.py
"max_grad_norm": 2.5658831600273806
```

**Analysis:**
- Gradient norms hitting 0.5 consistently suggests clipping at monitoring level
- Actual max_grad_norm is 2.57 but logged norms capped at 0.5
- This may indicate:
  1. Logging issue (norms normalized before logging)
  2. Actual gradients being clipped frequently
  3. Vanishing gradient problem

**Potential Issue:**
- If gradients genuinely this small, learning signal is weak
- May explain slow learning progress
- Could indicate poor signal propagation through deep networks

---

### ðŸŸ¡ MODERATE #8: Rollout Success Rate vs Episode Success Rate Mismatch

**Observation:**
```
rollout/success_rate: 47 data points (one per rollout)
episode/success_rate: 9,621 data points (one per episode)
```

Final values don't match - suggests different calculation methods or timing.

**Investigation Needed:**
- Confirm if rollout SR aggregates differently
- Check if evaluation episodes included
- Verify success criteria consistency

---

## 3. Reward Structure Analysis

### Current Reward Configuration

From `reward_constants.py` and config analysis:

```python
# Terminal rewards
LEVEL_COMPLETION_REWARD = 10.0
DEATH_PENALTY = -0.5
SWITCH_ACTIVATION_REWARD = 1.0

# Time penalty
TIME_PENALTY_PER_STEP = -0.0001

# PBRS - DISABLED
enable_pbrs = False

# Exploration rewards - ENABLED
enable_exploration_rewards = True
# Multi-scale: 0.01 per cell + 0.01 per 4x4 + 0.01 per 8x8 + 0.01 per 16x16 = 0.04 max per step
```

### Reward Balance Analysis

**Maximum Episode Reward (Success):**
```
Completion:         +10.0
Switch:             +1.0
Time penalty:       -2.0 (20k steps * -0.0001)
Exploration:        ~+100 (2500 cells * 0.04, overestimated)
Total:              ~+109.0 (in practice much less exploration)
```

**Typical Episode Reward (Failure):**
```
Death:              -0.5
Time penalty:       -0.5 to -2.0 (5k-20k steps)
Exploration:        ~+10-50 (wandering)
Total:              ~ -10 to +40
```

**Observed Mean Reward:** -0.0045 (essentially zero)

**Interpretation:**
- Mean near zero suggests balance between successes and failures
- But with 14% success rate, most episodes are failures
- Exploration rewards offsetting death penalties
- **Risk:** Agent may learn to maximize exploration instead of completion

---

## 4. Action Distribution Deep Dive

### Action Transition Matrices

TensorBoard logs 36 action transition probabilities (6x6 matrix). Key findings:

1. **High self-transitions:** Actions tend to persist (good for momentum)
2. **Low Jump â†’ Jump+Direction:** Suggests not combining jump with horizontal input
3. **NOOP traps:** High NOOP â†’ NOOP suggests indecision

### Movement Patterns

```
Movement Bias:
  Left:  52.8%
  Right: 47.2%
  (Nearly balanced - good)

Activity:
  Stationary: 15.5% (NOOP)
  Active:     84.5%
  (High activity - good)

Jump Frequency: 36.3%
  (Seems reasonable, but declining trend is bad)
```

### Concerning Trends

1. **Jump+Right collapsing:** 17.1% â†’ 8.5% (-50%)
   - This is often the most useful action (jump forward)
   - Suggests agent "unlearning" effective behavior

2. **Left/Right increasing:** Compensating for less jumping
   - Agent walking more, jumping less
   - Indicates shift toward safer, suboptimal policy

---

## 5. PPO Hyperparameter Assessment

### Current Configuration

```python
{
    "n_steps": 1024,           # âœ“ Good
    "batch_size": 256,         # âœ“ Good  
    "n_epochs": 5,             # âœ“ Standard
    "gamma": 0.999,            # âš ï¸ Very high (long-term)
    "gae_lambda": 0.998801,    # âš ï¸ Very high
    "clip_range": 0.3892,      # âš ï¸ Higher than standard (0.2)
    "clip_range_vf": 0.1,      # âœ“ OK
    "ent_coef": 0.00272,       # âŒ WAY too low (should be 0.01-0.1)
    "vf_coef": 0.469,          # âœ“ OK
    "max_grad_norm": 2.566,    # âœ“ OK
    "normalize_advantage": True, # âœ“ Good
    "learning_rate": 0.0003    # âœ“ Standard
}
```

### Issues Identified

1. **Entropy Coefficient Too Low**
   - Current: 0.00272
   - Recommended: 0.01 - 0.1 (4-40x higher)
   - Effect: Policy becoming deterministic too quickly

2. **Gamma Too High**
   - Current: 0.999
   - Standard: 0.99 - 0.995
   - Effect: Over-emphasizes long-term rewards
   - In sparse reward setting, this amplifies noise

3. **GAE Lambda Too High**
   - Current: 0.998801
   - Standard: 0.95 - 0.99
   - Effect: High variance advantage estimates
   - Couples with high gamma to create unstable learning

4. **Clip Range Slightly High**
   - Current: 0.389
   - Standard: 0.2 - 0.3
   - Effect: Larger policy updates (less conservative)
   - Not necessarily bad, but unusual

### Recommendations

```python
# Recommended changes:
"gamma": 0.995,              # Lower for better credit assignment
"gae_lambda": 0.97,          # Lower for less variance
"ent_coef": 0.02,            # 7x increase for exploration
"clip_range": 0.2,           # More conservative updates
```

---

## 6. Feature Extractor Analysis

### Current Architecture (MLP Baseline)

From `configurable_extractor.py` and config:

**Input Modalities:**
1. Player frame CNN: 84x84x1 grayscale â†’ 512 features
2. Global view CNN: 176x100x1 grayscale â†’ 256 features
3. Game state MLP: 26 features â†’ 128 features
4. Reachability MLP: 8 features â†’ 128 features
5. (Graph network disabled for MLP baseline)

**Fusion:** Multi-head attention (8 heads) â†’ 512 final features

**Total Parameters:** Likely 5-10M parameters

### Issues Identified

1. **High Complexity for Early Training**
   - Multi-modal attention is sophisticated
   - May be overfitting to simplest stages
   - Could benefit from simpler baseline first

2. **No Graph Network in Current Run**
   - Graph contains crucial spatial reasoning
   - MLP baseline may lack relational understanding
   - Could explain navigation difficulties

3. **Frame Stacking Disabled**
   - Config: `enable_visual_frame_stacking: false`
   - Temporal information lost
   - Agent can't infer velocity or momentum from single frame

### Strengths

1. **Reduced from 3D CNN**
   - 6.66x speedup over previous 3D CNN approach
   - 50% memory reduction
   - Good engineering decision

2. **Multi-scale visual processing**
   - Player frame (local) + Global view (context)
   - Complementary information

3. **Redundancy removed from game state**
   - Reduced from 55 to 19 node features
   - 26 game state features (optimized)
   - Shows thoughtful feature engineering

---

## 7. Pretraining Assessment

### Configuration

```python
"bc_epochs": 50,
"bc_batch_size": 128,
"no_pretraining": false,
"test_pretraining": false,
```

### Effectiveness Unknown

- No TensorBoard metrics for BC pretraining phase
- Cannot assess if pretraining helped or hurt
- Potential issues:
  1. Expert demos may not match RL policy needs
  2. BC can create strong priors that are hard to overcome
  3. 50 epochs may be too many (overfitting to demos)

### Hypothesis

- If BC demos are high-quality, should help initial stages
- Agent completed Stage 0 (simplest) at 100% â†’ pretraining worked there
- But failed to generalize to Stage 2 â†’ pretraining may create brittle policy

### Recommendation

- Log BC training metrics (loss, accuracy, action distribution)
- Compare BC action distribution vs RL action distribution
- Consider reducing BC epochs to 10-20 (light initialization only)

---

## 8. Curriculum Learning Failures

### Current Configuration

```python
"use_curriculum": true,
"curriculum_start_stage": "simplest",
"curriculum_threshold": 0.7,        # âš ï¸ High threshold
"curriculum_min_episodes": 100,     # âš ï¸ Not reached before stalling
"disable_trend_advancement": false,
"disable_early_advancement": false,
```

### Observed Behavior

**Stage Progression:**
```
Step   21,504: Stage 2, 0% SR, 0 episodes
Step  107,520: Stage 2, 25% SR, 4 episodes (peak early)
Step  301,056: Stage 2, 33% SR, 12 episodes (PEAK)
Step  645,120: Stage 2, 24% SR, 38 episodes (declining)
Step 1,010,688: Stage 2, 14% SR, 65 episodes (STALLED)
```

**Pattern:** Success rate peaked early, then declined as more episodes completed.

### Root Causes

1. **Threshold Too High**
   - 70% success in platformer is difficult
   - Especially with disabled PBRS
   - Agent needs dense rewards to reach 70%

2. **Exploration Decreased Over Time**
   - Entropy decreased
   - Jump actions decreased
   - Agent converged to suboptimal policy

3. **No Safety Net**
   - No automatic stage regression
   - No threshold annealing
   - No alternative advancement criteria

4. **Sample Inefficiency**
   - Only 65 episodes in stage 2 over 600k+ steps
   - Low sampling rate suggests long episodes
   - Episodes timing out (20k frame limit) without completion

### Comparison to RL Best Practices

**Recommended Curriculum Strategies:**
1. **Gradual threshold increase:** Start at 30%, increase to 70%
2. **Multiple advancement criteria:** Success rate OR improvement trend OR episode count
3. **Automatic regression:** Drop back if performance degrades
4. **Difficulty annealing:** Dynamically adjust difficulty within stage

**Current Implementation:** Fixed 70% threshold only

---

## 9. Literature Review: RL Best Practices

### Key Insights from Research

#### PPO Implementation Details (Huang et al., 2022)
- **13 core implementation details matter significantly**
- Entropy bonus critical for exploration
- Value function clipping helps stability
- Advantage normalization essential
- **Our implementation:** Missing optimal entropy schedule

#### Spinning Up (OpenAI)
- Policy gradients require careful tuning
- Baseline (value function) reduces variance
- Exploration-exploitation tradeoff crucial
- **Our case:** Premature exploitation due to low entropy

#### Curriculum Learning (Bengio et al., 2009)
- Start easy, gradually increase difficulty
- Adaptive pacing based on learner performance
- Avoid "forgetting" by revisiting earlier stages
- **Our case:** Too aggressive progression, no regression

#### PBRS Theory (Ng et al., 1999)
- Potential-based shaping preserves optimal policy
- Dense rewards accelerate learning
- Must use same gamma as RL algorithm
- **Our case:** PBRS disabled, losing critical benefits

### Specific Recommendations for Platformers

1. **Reward Shaping**
   - Dense rewards for navigation
   - Checkpoints/milestones for progress
   - Bonus for completing without deaths

2. **Exploration**
   - Higher entropy in early training
   - Intrinsic motivation (ICM, RND)
   - Curiosity-driven exploration

3. **Action Space**
   - Encourage jumping in early stages
   - Action masking for impossible actions
   - Auxiliary losses for action diversity

---

## 10. Recommendations Summary

### Immediate Critical Fixes (Priority 1)

1. **Enable PBRS** âœ“
   ```python
   "enable_pbrs": True,
   "pbrs_weights": {
       "objective_weight": 1.0,
       "hazard_weight": 0.1,
       "exploration_weight": 0.2,
   }
   ```

2. **Increase Entropy Coefficient** âœ“
   ```python
   "ent_coef": 0.02,  # 7x increase from 0.00272
   ```

3. **Lower Curriculum Threshold** âœ“
   ```python
   "curriculum_threshold": 0.5,  # Down from 0.7
   ```

4. **Extend Training Duration** âœ“
   ```python
   "total_timesteps": 10_000_000,  # 10x increase
   ```

### Important Fixes (Priority 2)

5. **Adjust PPO Hyperparameters** âœ“
   ```python
   "gamma": 0.995,        # Down from 0.999
   "gae_lambda": 0.97,    # Down from 0.9988
   "clip_range": 0.2,     # Down from 0.389
   ```

6. **Implement Adaptive Curriculum** âœ“
   - Start threshold at 0.3, increase to 0.6
   - Add trend-based advancement
   - Add stage regression if performance drops

7. **Add Action Regularization** âœ“
   - Bonus for action diversity
   - Penalty for avoiding jumps
   - Auxiliary loss for exploration

8. **Enable Frame Stacking** âœ“
   ```python
   "enable_visual_frame_stacking": True,
   "visual_stack_size": 4,
   ```

### Monitoring Improvements (Priority 3)

9. **Enhanced Logging** âœ“
   - BC pretraining metrics
   - Per-action Q-values
   - Exploration heatmaps
   - Success by level type

10. **Evaluation Protocol** âœ“
    - Separate eval environments
    - Fixed test levels
    - Record failure modes
    - Track action distribution over time

### Experimental Improvements (Priority 4)

11. **Intrinsic Motivation**
    - ICM (Intrinsic Curiosity Module)
    - RND (Random Network Distillation)
    - NGU (Never Give Up)

12. **Hierarchical RL**
    - High-level: plan route
    - Low-level: execute movement
    - Already partially implemented in codebase

13. **Simplified Architecture**
    - Test simpler feature extractor first
    - Enable graph network (spatial reasoning)
    - Ablation studies on modalities

---

## 11. Implementation Plan

### Phase 1: Critical Fixes (Week 1)

**Goals:**
- Enable PBRS
- Increase entropy coefficient
- Lower curriculum threshold
- Extend training to 10M timesteps

**Expected Results:**
- Agent should progress past Stage 2
- Jump actions should stabilize or increase
- Success rate should improve

**Success Metrics:**
- Reach Stage 3 (medium) within 5M timesteps
- Stage 2 success rate > 50%
- Jump+Direction actions > 20% combined

### Phase 2: Hyperparameter Tuning (Week 2)

**Goals:**
- Adjust gamma, GAE lambda, clip range
- Implement adaptive curriculum
- Add action regularization

**Expected Results:**
- More stable learning curves
- Better credit assignment
- Improved exploration

**Success Metrics:**
- Reach Stage 4 (exploration) within 10M timesteps
- Value estimates stabilize above -1.0
- Action entropy remains above 1.5

### Phase 3: Advanced Features (Week 3)

**Goals:**
- Enable frame stacking
- Implement intrinsic motivation
- Enhance logging and evaluation

**Expected Results:**
- Agent learns temporal patterns
- Exploration improves significantly
- Better debugging capabilities

**Success Metrics:**
- Complete 50%+ of Stage 5 (mine_heavy) levels
- Exploration coverage > 80% of level
- Clear visualizations of agent behavior

### Phase 4: Architecture Optimization (Week 4)

**Goals:**
- Enable graph network
- Simplify feature extractor
- Ablation studies

**Expected Results:**
- Better spatial reasoning
- Faster training
- Identify critical components

**Success Metrics:**
- Match or exceed baseline performance
- Training speed > 1000 FPS
- Clear understanding of component contributions

---

## 12. Detailed Metrics for Reference

### Complete Training Metrics

```
Curriculum Progression:
  Current Stage: 2 (simple)
  Success Rate: 0.140
  Episodes in Stage: 65
  Total Episodes: 201

Training Metrics:
  Explained Variance: 0.011 â†’ 0.529 (improving)
  Approx KL: 0.0087 â†’ 0.0121 (stable)
  Clip Fraction: 0.0605 â†’ 0.1324 (increasing)
  
Loss Metrics:
  Policy Loss: -0.00335 â†’ -0.00377
  Value Loss: 2.638 â†’ 0.015 (massive improvement)
  Entropy Loss: -1.785 â†’ -1.487 (decreasing entropy)
  Total Loss: 1.324 â†’ -0.0003

Action Distribution:
  NOOP: 15.2% â†’ 15.5%
  Left: 14.3% â†’ 22.3%
  Right: 17.1% â†’ 25.8%
  Jump: 13.3% â†’ 11.8%
  Jump+Left: 22.9% â†’ 16.0%
  Jump+Right: 17.1% â†’ 8.5% â¬‡ï¸

Rewards:
  Navigation (PBRS): 0.0000 (disabled)
  Exploration: 0.0000
  Total Mean: -0.0045

Value Estimates:
  Mean: -0.585 â†’ -2.756 (pessimistic)
  Std Dev: 0.173

Gradients:
  Total Norm: 0.489 (mean), 0.5 (max)
```

### Stage-by-Stage Performance

```
Stage 0 (simplest):
  Episodes: 65
  Success Rate: 100.0%
  Status: âœ“ Mastered

Stage 1 (simpler):
  Episodes: 71
  Success Rate: 64.0%
  Status: âœ“ Passed (threshold would be 50%)

Stage 2 (simple):
  Episodes: 65
  Success Rate: 14.0%
  Peak SR: 33.3% (step 301k)
  Status: âœ— Failed to advance

Stages 3-6:
  Never reached
```

---

## 13. Visualizations Needed

### Plots to Generate

1. **Training Curves**
   - Success rate over time (all stages)
   - Loss components over time
   - Entropy over time
   - Value estimates over time

2. **Action Analysis**
   - Action distribution evolution
   - Action transition heatmap
   - Jump frequency over time

3. **Curriculum Analysis**
   - Episodes per stage
   - Success rate per stage
   - Time spent in each stage

4. **Reward Analysis**
   - Reward components over time
   - Cumulative rewards per episode
   - Success vs failure reward distributions

5. **Exploration Analysis**
   - Spatial coverage heatmaps
   - Novel state visitation rate
   - Exploration bonus over time

---

## 14. Key Takeaways

### What's Working

1. âœ“ PPO training loop is stable (no crashes)
2. âœ“ Value function learning (EV 0.529)
3. âœ“ Parallel environments (21 envs)
4. âœ“ Pretraining enabled (BC from demos)
5. âœ“ Multi-modal observations (visual + state + reachability)
6. âœ“ Curriculum framework in place
7. âœ“ Comprehensive logging

### What's Broken

1. âœ— PBRS disabled (no dense rewards)
2. âœ— Entropy too low (premature convergence)
3. âœ— Curriculum too aggressive (can't progress)
4. âœ— Training too short (insufficient exploration)
5. âœ— Jump actions collapsing (wrong behavior learned)
6. âœ— Value estimates pessimistic (self-fulfilling failure)
7. âœ— No intrinsic motivation (no curiosity)

### Root Cause

The fundamental issue is **insufficient exploration** caused by:
1. Sparse rewards (PBRS disabled)
2. Low entropy (policy converged too fast)
3. Short training (not enough time)

These create a vicious cycle:
```
Sparse Rewards â†’ Random Wandering â†’ Frequent Deaths â†’ Negative Values â†’ 
Risk Aversion â†’ Jump Avoidance â†’ Cannot Complete Levels â†’ More Deaths â†’ ...
```

### Solution

Break the cycle by:
1. **Dense rewards** (PBRS) â†’ guide exploration
2. **High entropy** â†’ maintain exploration
3. **Long training** â†’ allow discovery
4. **Adaptive curriculum** â†’ match difficulty to capability

---

## 15. Conclusion

The current RL training setup has a solid foundation but is held back by a few critical configuration issues. The most impactful fixes are:

1. **Enable PBRS** - Single most important change
2. **Increase entropy coefficient** - Second most important
3. **Lower curriculum threshold** - Allow progression
4. **Extend training duration** - Give agent time to learn

With these changes, we expect the agent to:
- Progress through all curriculum stages
- Learn effective jumping mechanics
- Achieve >50% success rate on complex levels
- Generalize to unseen levels

The detailed analysis provides a clear roadmap for improvements and establishes metrics for tracking progress. All recommendations are based on RL best practices from peer-reviewed research and production RL systems.

---

## Appendix A: Research References

1. Schulman et al. (2017): "Proximal Policy Optimization Algorithms"
2. Ng et al. (1999): "Policy Invariance Under Reward Transformations"
3. Bengio et al. (2009): "Curriculum Learning"
4. Huang et al. (2022): "The 37 Implementation Details of Proximal Policy Optimization"
5. Pathak et al. (2017): "Curiosity-driven Exploration by Self-supervised Prediction"
6. OpenAI Spinning Up: Policy Gradient Methods
7. Sutton & Barto (2018): "Reinforcement Learning: An Introduction"

## Appendix B: Configuration Files

See:
- `training-results/config.json` - Main training configuration
- `npp_rl/agents/hyperparameters/ppo_hyperparameters.py` - PPO settings
- `nclone/gym_environment/reward_calculation/reward_constants.py` - Reward structure
- `npp_rl/training/architecture_configs.py` - Model architecture

## Appendix C: Contact

For questions about this analysis:
- Date: October 28, 2025
- Analyzer: OpenHands AI Agent
- Project: npp-rl (N++ Reinforcement Learning)
