{
  "_comment": "Example configuration for MLP with ICM (Independent of Hierarchical PPO)",
  
  "experiment_name": "mlp-icm-test-v1",
  "architectures": ["mlp_baseline"],
  
  "total_timesteps": 2000000,
  "num_envs": 128,
  "eval_freq": 500000,
  "save_freq": 1000000,
  
  "_comment_icm": "=== ICM CONFIGURATION ===",
  "enable_icm": true,
  "icm_config": {
    "feature_dim": 512,
    "action_dim": 6,
    "hidden_dim": 256,
    "eta": 0.01,
    "alpha": 0.1,
    "lambda_inv": 0.1,
    "lambda_fwd": 0.9,
    "learning_rate": 0.0001,
    "enable_mine_awareness": true,
    "r_int_clip": 1.0,
    "update_frequency": 4,
    "buffer_size": 10000,
    "debug": false
  },
  
  "_comment_explanation": "ICM Configuration Explained:",
  "_eta": "Intrinsic reward scaling factor (0.01 = moderate exploration)",
  "_alpha": "Weight for intrinsic rewards (0.1 = 10% intrinsic, 90% extrinsic)",
  "_lambda_inv": "Weight for inverse model loss (0.1 standard)",
  "_lambda_fwd": "Weight for forward model loss (0.9 standard)",
  "_enable_mine_awareness": "Use mine-aware curiosity to avoid dangerous exploration",
  "_r_int_clip": "Maximum intrinsic reward value to prevent instability",
  
  "_comment_frame_stacking": "=== FRAME STACKING (DISABLED FOR MLP) ===",
  "enable_visual_frame_stacking": false,
  "enable_state_stacking": false,
  
  "_comment_curriculum": "=== CURRICULUM LEARNING ===",
  "use_curriculum": true,
  "curriculum_threshold": 0.4,
  "curriculum_min_episodes": 100,
  
  "_comment_pbrs": "=== POTENTIAL-BASED REWARD SHAPING ===",
  "enable_pbrs": true,
  "pbrs_gamma": 0.99,
  "enable_mine_avoidance_reward": true,
  
  "_comment_hierarchical": "=== HIERARCHICAL PPO (NOT USED) ===",
  "use_hierarchical_ppo": false,
  "_note": "ICM works independently of hierarchical PPO in this config",
  
  "_comment_pretraining": "=== BEHAVIORAL CLONING PRETRAINING ===",
  "no_pretraining": false,
  "bc_epochs": 20,
  
  "_comment_hardware": "=== HARDWARE ===",
  "hardware_profile": "auto",
  "mixed_precision": true
}
