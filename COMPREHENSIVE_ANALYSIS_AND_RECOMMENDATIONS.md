# NPP-RL Training Analysis & Recommendations
## Comprehensive Analysis of Training Run: mlp-1029-f3-corridors-2

**Date:** October 30, 2025  
**Experiment:** mlp-1029-f3-corridors-2  
**Total Timesteps:** 1,000,000  
**Training Duration:** ~9 hours  
**Final Test Success Rate:** 0.0%

---

## Executive Summary

The agent demonstrates **significant learning on simple curriculum stages** (78.3% success on "simplest", 45% on "simpler") but **fails to generalize** to test environments or harder stages. The training shows clear signs of:

1. **Curriculum stagnation** - never advancing beyond "simple" stage
2. **Negative reward bias** - average episode reward of -40.26
3. **Inefficient exploration** - high NOOP action percentage (17.66%)
4. **Configuration inefficiencies** - frame stacking on MLP, low environment count
5. **Insufficient training budget** - 1M timesteps for complex task

**Critical Finding:** The agent learns to survive but not to complete objectives efficiently.

---

## Part 1: Detailed Performance Analysis

### 1.1 Curriculum Learning Performance

#### Success Rates by Stage
| Stage | Success Rate | Episodes | Avg Frames | Outcome |
|-------|-------------|----------|------------|---------|
| **simplest** | **78.3%** | 129 | 1,051 | ‚úÖ Good |
| **simpler** | **45.0%** | 191 | 2,795 | ‚ö†Ô∏è Moderate |
| **simple** | **26.6%** | 94 | 2,879 | ‚ùå Poor |
| medium | 0.0% | 0 | N/A | ‚ùå Never reached | 
| complex | 0.0% | 0 | N/A | ‚ùå Never reached |
| exploration | 0.0% | 0 | N/A | ‚ùå Never reached |
| mine_heavy | 0.0% | 0 | N/A | ‚ùå Never reached |

#### Top Performing Level Generators
1. **horizontal_corridor:minimal** - 92.7% success (38/41 episodes)
2. **vertical_corridor:minimal** - 73.9% success (34/46 episodes)
3. **corridors:simplest** - 69.0% success (29/42 episodes)
4. **single_chamber:obstacle** - 68.8% success (11/16 episodes)
5. **horizontal_corridor:simple** - 64.7% success (33/51 episodes)

#### Worst Performing Level Generators
1. **maze:tiny** - 8.3% success (1/12 episodes) - **Critical weakness**
2. **jump_required:simple** - 9.1% success (1/11 episodes) - **Critical weakness**
3. **vertical_corridor:platforms** - 9.1% success (1/11 episodes) - **Critical weakness**
4. **corridors:simple** - 11.7% success (7/60 episodes) - **Most common failure**
5. **single_chamber:gap** - 31.3% success (5/16 episodes)

### 1.2 Action Distribution Analysis

```
Action Frequencies:
- NOOP:        17.66% ‚Üê HIGH (agent often does nothing)
- Jump+Right:  19.77% ‚Üê Most common action
- Left:        16.80%
- Jump:        16.59%
- Jump+Left:   15.38%
- Right:       13.79%

Movement Patterns:
- Stationary:  17.66% ‚Üê Too high (should be <10%)
- Active:      82.34%
- Left bias:   48.96%
- Right bias:  51.04% ‚Üê Slight right preference

Jump Patterns:
- Jump frequency:        51.74% ‚Üê Very high (jumping over half the time)
- Directional jumps:     67.93% ‚Üê Good (mostly jumping with direction)
- Vertical-only jumps:   32.07%

Action Entropy: 1.782 ‚Üê Good exploration (max is ~1.79 for 6 actions)
```

**Analysis:**
- ‚úÖ Good exploration (high entropy)
- ‚úÖ Reasonable action diversity
- ‚ùå Too much inaction (17.66% NOOP)
- ‚ùå Excessive jumping (51.74%) suggests uncertainty about ground movement
- ‚ö†Ô∏è Action transitions show healthy variation

### 1.3 Reward Analysis

#### Hierarchical Rewards
```
Mean episode reward: -40.26 ¬± 19.09
Min episode reward:  -83.75
Max episode reward:  +6.05
Reward std:          148.64 ¬± 51.30 (very high variance)
```

#### PBRS (Potential-Based Reward Shaping) Components
```
Navigation rewards:   0.000039 ‚Üê Nearly zero contribution
Exploration rewards:  0.000096 ‚Üê Nearly zero contribution  
PBRS mean:           -0.004342 ‚Üê Negative! Should be closer to 0
Total mean:          -0.001933 ‚Üê Slightly negative
PBRS contribution:    99.4% ‚Üê Dominates reward signal
```

**Critical Issues:**
1. **Reward is predominantly negative** - Agent learns survival, not completion
2. **PBRS is negative on average** - Potential function may be misaligned
3. **Navigation/exploration rewards barely contribute** - Need amplification
4. **Massive reward variance** - Unstable learning signal

#### Reward Structure Problems

The reward structure appears to penalize the agent more than it rewards progress:

```python
# Current behavior (inferred):
Base completion reward: +1000 (only on level completion)
Frame penalty:          -0.1 per frame (5000 frames = -500)
Death penalty:          Large negative spike
PBRS:                   Mostly negative (potential decreasing)

# Result:
Average episode: -40.26 reward
Most episodes:   Timeout at 5000 frames with no completion
```

### 1.4 Learning Metrics

#### PPO Training Metrics
```
Learning Rate:        0.0003 (constant, no annealing)
Clip Fraction:        0.407 ¬± 0.097 ‚Üê HIGH (policy changing a lot)
Approx KL:            0.098 ¬± 0.049 ‚Üê Increasing trend
Explained Variance:   0.871 ¬± 0.200 ‚Üê Good (value function learning)

Policy Loss:         -0.028 ¬± 0.007
Value Loss:           0.032 ¬± 0.084
Entropy Loss:        -1.511 ¬± 0.135 ‚Üê Decreasing (less exploration over time)
Total Loss:          -0.035 ¬± 0.071
```

**Analysis:**
- ‚úÖ **Value function is learning well** (explained variance 0.87)
- ‚ö†Ô∏è **High clip fraction** (40.7%) - Policy updates are large
- ‚ö†Ô∏è **Increasing KL divergence** - Policy drifting from behavior policy
- ‚ùå **Entropy decreasing** - Exploration reducing too quickly
- ‚ùå **Negative total loss** - Unusual, may indicate optimization issues

#### Training Stability
```
Success rate trend:      DECREASING (started 53.7%, ended ~30-40%)
Episode length trend:    Many 5000-frame timeouts
Value estimates:         Mean -0.21, trend increasing (good)
Performance:             30.5 steps/sec, stable FPS
```

**Red Flag:** Success rate is **decreasing** over training, not increasing!

### 1.5 Curriculum Progression Analysis

#### Timeline
```
Stage timeline shows agent stuck between stages 1-2 (simpler and simple):
- Never reached stage 3+ (medium, complex, etc.)
- Curriculum threshold: 50% success rate
- Current stage success: Fluctuating around 30-50%
```

#### Curriculum Configuration Issues

```json
{
  "curriculum_threshold": 0.5,           ‚Üê May be too high
  "curriculum_min_episodes": 50,         ‚Üê OK
  "disable_early_advancement": false,    ‚Üê Good
  "disable_trend_advancement": false     ‚Üê Good
}
```

**Problems:**
1. **Threshold too strict** - 50% might be too high for this task
2. **Agent oscillates** around threshold without stable improvement
3. **No fallback mechanism** - Can't drop back to easier stages when struggling
4. **Limited stage diversity** - Most training on "simpler" stage (191 episodes)

---

## Part 2: Root Cause Analysis

### 2.1 Why Isn't the Agent Learning to Complete Levels?

#### Primary Issues

**1. Reward Structure Misalignment**
```
Current: Large positive on completion, small negative per frame, huge negative on death
Problem: Agent learns "don't die" instead of "complete quickly"
Impact:  Slow, cautious behavior that times out
```

**2. Sparse Reward Problem**
```
Current: Reward only when completing level
Problem: No intermediate feedback for progress
Impact:  Random exploration without guidance
```

**3. PBRS Implementation Issues**
```
Current: PBRS mean is -0.0043 (negative!)
Problem: Potential function likely uses distance-to-goal which decreases slowly
Impact:  Agent doesn't get clear progress signal
```

**4. Timeout Too Generous**
```
Current: 5000 frames (83 seconds) per level
Problem: Agent learns to wander without urgency
Impact:  Many episodes hit timeout
```

### 2.2 Why Can't the Agent Generalize?

**1. Insufficient Training Budget**
```
Current: 1M timesteps
Typical: 10M-100M timesteps for complex environments
Impact:  Not enough experience to generalize
```

**2. Limited Environment Diversity**
```
Current: 28 parallel environments
Recommended: 64-256 for PPO
Impact:  Limited state coverage
```

**3. Frame Stacking on MLP**
```
Current: 3-frame visual stacking enabled
Problem: MLP doesn't benefit from temporal information
Impact:  4x memory usage, no benefit, slower training
```

**4. Curriculum Never Advances**
```
Current: Stuck on simple/simpler stages
Problem: Never sees diverse level types
Impact:  Overfits to simple corridors
```

### 2.3 Why High NOOP Percentage?

**1. Uncertainty in Policy**
```
High NOOP (17.66%) suggests agent doesn't know what to do
Likely causes:
- Poor value estimates for actions
- Sparse reward ‚Üí no clear "good" action
- High variance in outcomes
```

**2. Local Minima**
```
Agent may have learned:
- "Standing still is safe" (no death penalty)
- "Moving might hit mine" (death penalty)
- "Not sure where to go" (no navigation signal)
```

### 2.4 Why Negative Average Rewards?

**Mathematical Analysis:**
```python
# Typical episode that fails:
frames_taken = 5000
frame_penalty = -0.1 * 5000 = -500
completion_bonus = 0  (didn't finish)
death_penalty = -100 (if died)
pbrs_reward = -0.0043 * 5000 = -21.5

total = -500 + 0 - 100 - 21.5 = -621.5

# But we see average of -40.26, which means:
# Some episodes complete (get +1000)
# Most episodes fail (get large negative)
# Average is dominated by failures
```

---

## Part 3: Actionable Recommendations

### Priority 1: Critical Fixes (Implement Immediately)

#### 3.1 Fix Reward Structure üî• **HIGHEST PRIORITY**

**Current Problem:** Negative rewards dominate, agent learns survival not completion.

**Recommended Changes:**

```python
# Current (inferred):
reward = {
    'completion': +1000,
    'per_frame': -0.1,
    'death': -100,
}

# Recommended:
reward = {
    'completion': +1000,
    'per_frame': -0.01,  # Reduce by 10x
    'death': -10,         # Reduce by 10x
    'exit_switch_touched': +100,  # NEW: Reward sub-objective
    'progress_to_switch': +0.1,   # NEW: Reward approaching switch
    'progress_to_exit': +0.1,     # NEW: Reward approaching exit (after switch)
}

# Or even better: Normalize to [0, 1] scale
reward = {
    'completion': +1.0,
    'per_frame': -0.0001,
    'switch_touched': +0.5,
    'distance_to_objective': potential_difference,  # PBRS
}
```

**Implementation Location:** `nclone/nclone/gym_environment/npp_environment.py`

**Code Changes:**
```python
# In npp_environment.py, modify _calculate_reward():

def _calculate_reward(self):
    reward = 0.0
    
    # Completion reward (normalized)
    if self.player_won:
        reward += 1.0
    
    # Small time penalty (10x smaller than before)
    reward -= 0.0001  # Was -0.001
    
    # Death penalty (normalized)
    if self.ninja.state == NinjaState.DEAD:
        reward -= 0.1  # Was -1.0
    
    # NEW: Sub-objective rewards
    if not self._switch_was_touched and self.switch_touched:
        reward += 0.5  # Major milestone
        self._switch_was_touched = True
    
    return reward
```

#### 3.2 Fix PBRS Implementation üî•

**Current Problem:** PBRS is negative on average, hurting learning.

**Diagnosis:**
```python
# PBRS formula: R_t = Œ≥ * Œ¶(s_{t+1}) - Œ¶(s_t)
# If agent gets farther from goal: Œ¶ decreases ‚Üí R is negative
# Agent might be wandering, making PBRS always negative
```

**Recommended Fixes:**

1. **Check potential function bounds:**
```python
# In subtask_rewards.py or similar:

def calculate_potential(self, state):
    # Ensure potential is always positive
    dist_to_goal = state['distance_to_objective']
    max_dist = 1500  # Maximum possible distance
    
    # Normalize to [0, 1]
    potential = 1.0 - (dist_to_goal / max_dist)
    potential = max(0.0, min(1.0, potential))  # Clamp
    
    return potential
```

2. **Verify gamma is correct:**
```python
# Current: gamma = 0.995
# This might be too high, try 0.99 or 0.98
pbrs_gamma = 0.99  # Slightly more near-sighted
```

3. **Add debugging:**
```python
# Log PBRS components
if self.log_reward_components:
    info['pbrs_potential_curr'] = potential_curr
    info['pbrs_potential_next'] = potential_next
    info['pbrs_reward'] = pbrs_reward
    info['pbrs_is_positive'] = pbrs_reward > 0
```

#### 3.3 Remove Frame Stacking for MLP üî•

**Current Problem:** Frame stacking adds 4x memory cost with no benefit for MLP.

**Implementation:**
```json
// In config.json:
{
  "enable_visual_frame_stacking": false,  // CHANGE: was true
  "visual_stack_size": 1,                 // CHANGE: was 3
}
```

**Expected Impact:**
- **4x reduction** in observation size
- **Faster training** (less data to process)
- **Same performance** (MLP doesn't use temporal info)
- Can increase batch size or add more environments

#### 3.4 Increase Environment Count üî•

**Current Problem:** Only 28 environments ‚Üí limited state diversity.

**Recommended Change:**
```json
{
  "num_envs": 128,  // CHANGE: was 28
}
```

**Rationale:**
- PPO paper uses 128+ environments
- More environments ‚Üí better state coverage
- More stable gradient estimates
- Faster convergence

**Hardware Check:**
```python
# Your hardware: 1x A100-SXM4-40GB (42GB)
# With frame stacking removed:
#   Current: 28 envs * 3 frames * ~2MB = ~168MB
#   New:     128 envs * 1 frame * ~2MB = ~256MB
# Plenty of headroom!
```

### Priority 2: Important Improvements

#### 3.5 Adjust Curriculum Settings

**Current Problem:** Threshold too high, agent can't advance.

**Recommended Changes:**
```json
{
  "curriculum_threshold": 0.4,        // CHANGE: was 0.5 (Lower threshold)
  "curriculum_min_episodes": 100,     // CHANGE: was 50 (More episodes per stage)
  "curriculum_success_window": 100    // ADD: Rolling window for success rate
}
```

**Add Adaptive Curriculum:**
```python
# In curriculum_manager.py:

class CurriculumManager:
    def should_advance(self):
        # Current: Fixed 50% threshold
        # Recommended: Adaptive threshold
        
        if self.episodes_in_stage > 200:
            # Force advancement after 200 episodes
            return True
        
        if self.success_rate > self.threshold:
            return True
        
        if self.episodes_in_stage > 100 and self.success_rate > 0.3:
            # Relaxed threshold after 100 episodes
            return True
        
        return False
```

#### 3.6 Increase Training Budget

**Current Problem:** 1M timesteps insufficient for complex task.

**Recommended Changes:**
```json
{
  "total_timesteps": 10000000,  // CHANGE: was 1000000 (10x increase)
  "eval_freq": 500000,          // CHANGE: was 100000 (evaluate less often)
  "save_freq": 1000000,         // CHANGE: was 500000 (save less often)
}
```

**Justification:**
- Complex environments need 10M-100M timesteps
- Your task has:
  - Large state space (full level visibility)
  - Complex physics (momentum, wall sliding, jumping)
  - Multiple objectives (switch ‚Üí exit)
  - Many level types (7 curriculum stages)
- Expect meaningful results after 5-10M timesteps

#### 3.7 Reduce Episode Timeout

**Current Problem:** 5000 frames too generous, agent learns to dawdle.

**Recommended Change:**
```python
# In environment config:
max_frames_per_episode = 2500  # CHANGE: was 5000

# Or dynamic based on curriculum:
timeouts = {
    'simplest': 1000,
    'simpler': 1500,
    'simple': 2000,
    'medium': 2500,
    'complex': 3000,
    'exploration': 3500,
    'mine_heavy': 3000,
}
```

**Rationale:**
- Current: 83 seconds per level (way too long)
- Typical human: 5-30 seconds
- Tighter timeout ‚Üí urgency ‚Üí faster completion
- Prevents aimless wandering

#### 3.8 Add Dense Reward Shaping

**Recommended Additions:**

```python
# 1. Distance-based rewards
reward += self._calculate_distance_reward(state, prev_state)

def _calculate_distance_reward(self, state, prev_state):
    """Reward for getting closer to objective."""
    if not self.switch_touched:
        # Reward approaching switch
        prev_dist = prev_state['distance_to_switch']
        curr_dist = state['distance_to_switch']
        return 0.001 * (prev_dist - curr_dist)  # Positive if closer
    else:
        # Reward approaching exit
        prev_dist = prev_state['distance_to_exit']
        curr_dist = state['distance_to_exit']
        return 0.001 * (prev_dist - curr_dist)

# 2. Exploration reward (NEW tiles visited)
reward += self._calculate_exploration_reward(state)

def _calculate_exploration_reward(self, state):
    """Reward for visiting new tiles."""
    pos = (int(state['ninja_x'] / 24), int(state['ninja_y'] / 24))
    if pos not in self.visited_tiles:
        self.visited_tiles.add(pos)
        return 0.01  # Small bonus for new tile
    return 0.0

# 3. Velocity reward (encourage movement)
reward += self._calculate_velocity_reward(state)

def _calculate_velocity_reward(self, state):
    """Small reward for moving (discourages NOOP)."""
    speed = np.sqrt(state['vx']**2 + state['vy']**2)
    return 0.0001 * speed  # Tiny bonus for movement
```

### Priority 3: Advanced Optimizations

#### 3.9 Implement Reward Normalization

**Add reward wrapper:**
```python
from stable_baselines3.common.vec_env import VecNormalize

# In training script:
env = make_vec_env(...)
env = VecNormalize(
    env,
    norm_obs=True,           # Normalize observations
    norm_reward=True,        # IMPORTANT: Normalize rewards!
    clip_obs=10.0,
    clip_reward=10.0,
    gamma=0.99,
)
```

**Why:**
- Normalizes reward scale to ~[-1, +1]
- Prevents large reward spikes from dominating
- More stable learning
- Better generalization

#### 3.10 Tune PPO Hyperparameters

**Current Settings:**
```json
{
  "batch_size": 256,
  "n_steps": 1024,
  "learning_rate": 0.0003,
}
```

**Recommended Tuning:**
```json
{
  "batch_size": 512,           // INCREASE: More stable gradients
  "n_steps": 2048,             // INCREASE: Longer rollouts
  "learning_rate": 0.0003,     // KEEP: Standard PPO rate
  "n_epochs": 10,              // ADD: Multiple optimization epochs
  "gamma": 0.99,               // ADD: Discount factor
  "gae_lambda": 0.95,          // ADD: GAE for advantage estimation
  "clip_range": 0.2,           // KEEP: Standard clip range
  "ent_coef": 0.01,            // ADD: Entropy coefficient for exploration
  "vf_coef": 0.5,              // ADD: Value function coefficient
  "max_grad_norm": 0.5,        // ADD: Gradient clipping
}
```

#### 3.11 Enable Learning Rate Annealing

**Current:** Constant learning rate (0.0003)

**Recommended:**
```python
# Linear annealing
def lr_schedule(progress_remaining):
    """
    Progress_remaining: 1.0 at start, 0.0 at end
    """
    return 0.0003 * progress_remaining

model = PPO(
    policy,
    env,
    learning_rate=lr_schedule,  # CHANGE: Function instead of float
    ...
)
```

**Or enable in config:**
```json
{
  "enable_lr_annealing": true,   // CHANGE: was false
  "initial_lr": 0.0003,
  "final_lr": 0.00003,           // 10x smaller at end
}
```

#### 3.12 Implement Intrinsic Motivation

**Add curiosity-driven exploration:**

```python
# Option 1: ICM (Intrinsic Curiosity Module)
from npp_rl.intrinsic.icm import ICM

icm = ICM(
    observation_space=env.observation_space,
    action_space=env.action_space,
    feature_dim=256,
    inverse_weight=0.2,
    forward_weight=0.8,
)

# Option 2: Random Network Distillation (RND)
# Option 3: Count-based exploration

# Add intrinsic reward to environment wrapper
intrinsic_reward = icm.compute_intrinsic_reward(obs, action, next_obs)
total_reward = extrinsic_reward + 0.01 * intrinsic_reward
```

**Why:**
- Encourages exploration of novel states
- Helps overcome sparse rewards
- May discover better strategies

#### 3.13 Add Auxiliary Tasks

**Recommended additions:**

```python
# 1. State prediction
# Train network to predict next state from current state + action
# Helps learn environment dynamics

# 2. Value auxiliary task
# Additional value heads for sub-goals (switch, exit)
# Improves value function learning

# 3. Pixel control
# Predict which pixels will change (if using vision)
# Better representation learning
```

---

## Part 4: Implementation Roadmap

### Phase 1: Critical Fixes (Week 1)
**Goal:** Fix fundamental issues blocking learning

1. ‚úÖ **Day 1-2: Fix reward structure**
   - Reduce frame penalty 10x
   - Add switch-touched reward
   - Normalize reward scale
   - Test on simple levels

2. ‚úÖ **Day 2-3: Fix PBRS**
   - Debug potential function
   - Ensure non-negative potentials
   - Verify gamma setting
   - Add logging

3. ‚úÖ **Day 3-4: Configuration fixes**
   - Remove frame stacking
   - Increase environments to 128
   - Reduce episode timeout to 2500
   - Update curriculum threshold to 0.4

4. ‚úÖ **Day 4-5: Test run**
   - Train for 2M timesteps
   - Monitor reward trends
   - Verify improvements
   - Adjust if needed

**Expected Outcomes:**
- Positive average rewards
- Higher success rates on simple levels
- Agent moves more (less NOOP)
- Curriculum starts advancing

### Phase 2: Important Improvements (Week 2)
**Goal:** Enable better generalization

1. ‚úÖ **Day 6-7: Curriculum improvements**
   - Implement adaptive thresholds
   - Add success window tracking
   - Force advancement after N episodes

2. ‚úÖ **Day 7-8: Dense reward shaping**
   - Add distance-based rewards
   - Add exploration bonuses
   - Add velocity encouragement

3. ‚úÖ **Day 8-9: Increase training budget**
   - Set total_timesteps to 10M
   - Run longer training
   - Monitor for 24+ hours

4. ‚úÖ **Day 9-10: Reward normalization**
   - Add VecNormalize wrapper
   - Test on all curriculum stages

**Expected Outcomes:**
- Curriculum advances to medium/complex
- Success rate >50% on simple
- Test set performance >0%
- Faster episode completions

### Phase 3: Advanced Optimizations (Week 3)
**Goal:** Maximize performance

1. ‚úÖ **Day 11-12: Hyperparameter tuning**
   - Increase batch size to 512
   - Increase n_steps to 2048
   - Add learning rate annealing

2. ‚úÖ **Day 12-13: Intrinsic motivation**
   - Implement ICM or RND
   - Tune intrinsic reward coefficient

3. ‚úÖ **Day 13-14: Auxiliary tasks**
   - Add state prediction
   - Add value auxiliary heads

4. ‚úÖ **Day 14-15: Final evaluation**
   - Train for 20M+ timesteps
   - Comprehensive test evaluation
   - Ablation studies

**Expected Outcomes:**
- Success rate >70% on all stages
- Test set performance >50%
- Fast, efficient completions
- Strong generalization

---

## Part 5: Monitoring & Evaluation

### Key Metrics to Track

**1. Success Rates (Most Important)**
```
‚úÖ Target: 
  - Simplest: >90%
  - Simpler:  >80%
  - Simple:   >70%
  - Medium:   >60%
  - Complex:  >50%
  - Test set: >50%

‚ùå Current:
  - Simplest: 78.3%
  - Simpler:  45.0%
  - Simple:   26.6%
  - Test set: 0.0%
```

**2. Reward Trends**
```
‚úÖ Target:
  - Average episode reward: >0 (positive!)
  - Reward trend: Increasing over time
  - PBRS contribution: 20-40% (not 99%!)
  
‚ùå Current:
  - Average: -40.26
  - Trend: Stable (not improving)
  - PBRS: 99.4% (too dominant)
```

**3. Episode Efficiency**
```
‚úÖ Target:
  - Average frames < 1000 (simple levels)
  - Timeout rate < 10%
  
‚ùå Current:
  - Average frames: 2800+ (simple)
  - Many timeouts
```

**4. Action Distribution**
```
‚úÖ Target:
  - NOOP < 10%
  - Balanced left/right
  - Jump ~30-40%
  
‚ùå Current:
  - NOOP: 17.66% (too high)
  - Jump: 51.74% (too high)
```

**5. Learning Stability**
```
‚úÖ Target:
  - Explained variance > 0.8
  - Clip fraction < 0.3
  - KL divergence < 0.05
  
‚ö†Ô∏è Current:
  - Explained variance: 0.87 ‚úì
  - Clip fraction: 0.41 ‚úó
  - KL divergence: 0.098 ‚úó
```

### Evaluation Protocol

**Every 500K timesteps:**

1. **Checkpoint model**
2. **Run evaluation suite** (100 episodes per stage)
3. **Record metrics:**
   - Success rates by stage
   - Average episode length
   - Average reward
   - Action distribution
4. **Generate visualizations:**
   - Route images for successes
   - Route images for failures
   - Learning curves
   - Reward distributions
5. **Analyze failures:**
   - Which level types fail?
   - What behaviors lead to failure?
   - Are there systematic errors?

### Debugging Checklist

**If success rate isn't improving:**
- [ ] Check reward is positive on average
- [ ] Check PBRS isn't negative
- [ ] Check timeout isn't too generous
- [ ] Check curriculum is advancing
- [ ] Check value function is learning (explained variance)
- [ ] Check entropy isn't too low (exploration)

**If agent gets stuck:**
- [ ] Reduce curriculum threshold
- [ ] Add more dense rewards
- [ ] Increase exploration (entropy coefficient)
- [ ] Check for local minima in reward function

**If training is unstable:**
- [ ] Reduce learning rate
- [ ] Increase batch size
- [ ] Add gradient clipping
- [ ] Normalize rewards (VecNormalize)

---

## Part 6: Comparative Analysis

### How Does This Compare to SOTA?

**Successful RL systems typically have:**

| Metric | Successful RL | Your System | Status |
|--------|--------------|-------------|--------|
| Training timesteps | 10M-100M | 1M | ‚ùå Too low |
| Parallel envs | 64-256 | 28 | ‚ùå Too low |
| Average reward | Positive & increasing | -40 | ‚ùå Negative |
| Success rate | >50% | 0% (test) | ‚ùå Failed |
| Curriculum stages | Progressively harder | Stuck at stage 2 | ‚ùå Stuck |
| Reward design | Dense, shaped | Sparse, penalties | ‚ùå Poor |
| Exploration | Curiosity, entropy | Entropy only | ‚ö†Ô∏è Basic |

### Lessons from Similar Domains

**Platformer games (similar to N++):**

1. **Sparse rewards don't work** 
   - Montezuma's Revenge: Required curiosity-driven exploration
   - Solution: Dense rewards, PBRS, count-based exploration

2. **Timeout matters**
   - Sonic the Hedgehog: Tight time limits crucial
   - Solution: Shorter episodes force efficient behavior

3. **Curriculum essential**
   - VizDoom, Obstacle Tower: Curriculum learning key
   - Solution: Gradual difficulty increase

4. **Vision helps but not required**
   - Your system has full state info (better than vision!)
   - Many games use extracted features like yours

**Your advantages:**
- ‚úÖ Full level visibility (no partial observability)
- ‚úÖ Exact positions (no noise)
- ‚úÖ Deterministic physics
- ‚úÖ Clear objectives (switch ‚Üí exit)

**So why isn't it working?**
- ‚ùå Reward structure is fundamentally broken
- ‚ùå Training budget too small
- ‚ùå Configuration inefficiencies

---

## Part 7: Expected Outcomes After Fixes

### Realistic Expectations

**After Phase 1 (Critical Fixes):**
```
Timesteps: 2M
Timeline: 2-3 days training
Expected:
  - Average reward: Positive (+5 to +20)
  - Success rate (simplest): >85%
  - Success rate (simpler): >60%
  - Success rate (simple): >40%
  - Test set: >5%
  - Curriculum: Advancing to medium
```

**After Phase 2 (Important Improvements):**
```
Timesteps: 10M
Timeline: 1-2 weeks training
Expected:
  - Average reward: +20 to +50
  - Success rate (all stages): >50%
  - Test set: >30%
  - Curriculum: Reaching complex
  - Episode length: <1500 frames avg
```

**After Phase 3 (Advanced Optimizations):**
```
Timesteps: 20M+
Timeline: 2-4 weeks training
Expected:
  - Average reward: +50 to +100
  - Success rate (all stages): >70%
  - Test set: >60%
  - Curriculum: Mastering all stages
  - Episode length: <1000 frames avg
  - Near-human performance
```

### Success Criteria

**Minimum Viable Performance:**
- ‚úÖ Test set success >25%
- ‚úÖ Average reward positive
- ‚úÖ Curriculum advances through all stages
- ‚úÖ NOOP < 10%

**Good Performance:**
- ‚úÖ Test set success >50%
- ‚úÖ Success >60% on all curriculum stages
- ‚úÖ Episodes complete in <2000 frames
- ‚úÖ Stable learning (no catastrophic forgetting)

**Excellent Performance:**
- ‚úÖ Test set success >70%
- ‚úÖ Success >80% on all curriculum stages
- ‚úÖ Episodes complete in <1000 frames
- ‚úÖ Generalizes to unseen level types

---

## Part 8: Code Changes Summary

### Files to Modify

**1. Reward Structure** (`nclone/nclone/gym_environment/npp_environment.py`)
```python
# Reduce penalties, add dense rewards
# Normalize to reasonable scale
# Add switch-touched milestone reward
```

**2. PBRS** (`npp_rl/wrappers/hierarchical_reward_wrapper.py` or `npp_rl/hrl/subtask_rewards.py`)
```python
# Fix potential function to ensure non-negative
# Verify gamma = 0.99
# Add bounds checking
# Add debugging logs
```

**3. Configuration** (`config.json` or training script)
```json
{
  "enable_visual_frame_stacking": false,
  "num_envs": 128,
  "total_timesteps": 10000000,
  "curriculum_threshold": 0.4,
  "max_episode_frames": 2500,
}
```

**4. Training Script** (`npp_rl/training/architecture_trainer.py`)
```python
# Add VecNormalize wrapper
# Add learning rate annealing
# Tune PPO hyperparameters
```

**5. Curriculum Manager** (`npp_rl/training/curriculum_manager.py`)
```python
# Add adaptive threshold
# Add forced advancement
# Add success window tracking
```

### Testing Strategy

**1. Unit Tests**
```python
# Test reward function returns correct values
# Test PBRS potential is always positive
# Test curriculum advancement logic
```

**2. Integration Tests**
```python
# Test full episode with new rewards
# Test curriculum progresses correctly
# Test agent trains without errors
```

**3. Validation Runs**
```python
# Short runs (100K steps) to verify:
#   - Rewards are positive
#   - Agent learns simple levels
#   - No crashes or errors
```

---

## Part 9: Additional Considerations

### Computational Budget

**Current Run:**
- Time: ~9 hours for 1M timesteps
- Hardware: 1x A100 (42GB)
- Cost: ~$10-15 (if cloud)

**Recommended Runs:**
- 10M timesteps: ~90 hours (~4 days)
- 20M timesteps: ~180 hours (~8 days)
- Cost: ~$100-200 per full run

**Optimization suggestions:**
- Remove frame stacking: ~30% faster
- Increase batch size: More GPU utilization
- Enable mixed precision: Already enabled ‚úì
- Profile for bottlenecks: Use `py-spy` or `cProfile`

### Alternative Approaches

If current approach still struggles after fixes:

**1. Imitation Learning Bootstrap**
- Record more human demonstrations
- Pre-train on BC for longer (>20 epochs)
- Use behavioral cloning loss as auxiliary task

**2. Hierarchical RL**
- High-level policy: Choose sub-goal (switch/exit)
- Low-level policy: Navigate to sub-goal
- Easier credit assignment

**3. Model-Based RL**
- Learn dynamics model
- Plan with model
- More sample efficient

**4. Ensemble Methods**
- Train multiple agents
- Ensemble predictions
- More robust

### Architecture Alternatives

Currently using MLP baseline. Consider:

**1. GNN (Graph Neural Network)**
- Reason about spatial relationships
- Better generalization
- More parameters (slower)

**2. Attention Mechanisms**
- Focus on relevant entities
- Handle variable entities
- Better for complex levels

**3. Memory-Augmented Networks**
- Remember visited locations
- Better exploration
- Handle partial observability

**Your system already supports these!** See:
- `npp_rl/models/gat.py` (Graph Attention)
- `npp_rl/models/hgt_encoder.py` (Heterogeneous Graph)
- `npp_rl/models/attention_mechanisms.py`

**Recommendation:** Fix MLP first, then try GNN if needed.

---

## Part 10: Conclusion

### Summary of Findings

**The Good:**
- ‚úÖ Agent learns basic survival skills
- ‚úÖ 78% success on simplest levels
- ‚úÖ Value function learning well
- ‚úÖ Good exploration (high entropy)
- ‚úÖ Reasonable action diversity

**The Bad:**
- ‚ùå 0% success on test set (critical failure)
- ‚ùå Negative average rewards
- ‚ùå Curriculum stuck at stage 2
- ‚ùå High NOOP percentage
- ‚ùå Many timeout failures

**The Fixable:**
- üîß Reward structure can be fixed
- üîß PBRS can be debugged
- üîß Configuration can be optimized
- üîß Training budget can be increased
- üîß Curriculum can be adjusted

### Root Cause

**The fundamental problem is reward design.**

The agent is learning exactly what you're teaching it:
- Don't die (large death penalty) ‚úì
- Don't move much (time penalty) ‚úì
- Wander around (no direction) ‚úì

But NOT:
- Complete levels quickly ‚úó
- Find the switch ‚úó
- Reach the exit ‚úó

Fix the reward structure and everything else will improve.

### Confidence in Recommendations

**High Confidence (>90%):**
- Fix reward structure ‚Üí will improve
- Remove frame stacking ‚Üí will speed up
- Increase environments ‚Üí will help generalization
- Increase training budget ‚Üí will reach better performance

**Medium Confidence (70-90%):**
- Curriculum changes ‚Üí should help progression
- PBRS fixes ‚Üí should provide better signal
- Timeout reduction ‚Üí should encourage efficiency
- Dense rewards ‚Üí should guide learning

**Low Confidence (<70%):**
- Intrinsic motivation ‚Üí might help, hard to tune
- Architecture changes ‚Üí probably not needed
- Model-based RL ‚Üí overkill for this problem

### Next Steps

**Immediate (this week):**
1. Implement Priority 1 fixes (reward, PBRS, config)
2. Run test training for 2M timesteps
3. Verify improvements in metrics
4. Iterate on reward function if needed

**Short-term (next 2 weeks):**
1. Implement Priority 2 improvements
2. Run full 10M timestep training
3. Evaluate on test set
4. Analyze failure modes

**Long-term (next month):**
1. Implement Priority 3 optimizations if needed
2. Run 20M+ timestep training
3. Publish results
4. Try advanced architectures (GNN)

### Final Thoughts

**This is a solvable problem.**

The infrastructure is solid:
- Good simulation
- Reasonable observations
- Proper PPO implementation
- Curriculum system in place
- Good logging and visualization

The issue is **reward engineering** - a common challenge in RL.

With the recommended fixes, I expect:
- **2M timesteps:** Positive results, curriculum advancing
- **10M timesteps:** >50% test set performance
- **20M timesteps:** >70% test set performance

**You're closer than you think.** The agent is learning, just not what you want it to learn. Fix the reward, and watch it succeed.

---

## Appendix A: Reward Function Pseudocode

### Current (Inferred)
```python
def calculate_reward():
    if completed_level:
        return 1000.0
    if died:
        return -100.0
    return -0.1  # Per frame penalty
```

### Recommended v1 (Quick Fix)
```python
def calculate_reward():
    reward = 0.0
    
    # Major events
    if completed_level:
        reward += 1.0
    if died:
        reward -= 0.1
    if switch_just_touched:
        reward += 0.5
    
    # Dense shaping
    reward -= 0.0001  # Tiny time penalty
    reward += 0.001 * distance_improvement  # Progress reward
    
    return reward
```

### Recommended v2 (Better)
```python
def calculate_reward():
    reward = 0.0
    
    # Completion (scaled)
    if completed_level:
        time_bonus = max(0, 1.0 - steps / max_steps)
        reward += 1.0 + 0.5 * time_bonus
    
    # Death (scaled)
    if died:
        reward -= 0.1
    
    # Sub-objectives
    if switch_just_touched:
        reward += 0.5
    
    # Dense rewards
    if not switch_touched:
        # Distance to switch
        dist_reward = (prev_dist_switch - curr_dist_switch) / max_dist
        reward += 0.1 * dist_reward
    else:
        # Distance to exit
        dist_reward = (prev_dist_exit - curr_dist_exit) / max_dist
        reward += 0.1 * dist_reward
    
    # Exploration
    if visited_new_tile:
        reward += 0.01
    
    # Movement encouragement
    reward += 0.0001 * velocity
    
    # Time pressure (small)
    reward -= 0.0001
    
    return reward
```

### Recommended v3 (Advanced)
```python
def calculate_reward():
    reward = 0.0
    
    # Sparse rewards (1.0 scale)
    if completed_level:
        reward += 1.0
        reward += 0.5 * (1.0 - steps / max_steps)  # Time bonus
    
    if died:
        reward -= 0.1
    
    if switch_just_touched:
        reward += 0.5
    
    # PBRS (potential-based)
    if not switch_touched:
        potential = 1.0 - (dist_to_switch / max_dist)
    else:
        potential = 1.0 - (dist_to_exit / max_dist)
    
    pbrs_reward = gamma * potential_next - potential_prev
    reward += 0.5 * pbrs_reward  # Weight PBRS at 50%
    
    # Exploration bonus
    novelty = count_based_bonus(state)
    reward += 0.01 * novelty
    
    # Action shaping
    if action == NOOP:
        reward -= 0.001  # Penalize inaction
    
    if dangerous_situation() and safe_action(action):
        reward += 0.01  # Reward smart avoidance
    
    # Normalize to reasonable scale
    reward = np.clip(reward, -1.0, 2.0)
    
    return reward
```

---

## Appendix B: Monitoring Dashboard

### Real-time Metrics (TensorBoard)

**Success Metrics:**
```
- rollout/success_rate
- episode/success_rate_smoothed
- curriculum/success_rate
- curriculum_stages/*_success_rate
```

**Reward Metrics:**
```
- rewards/hierarchical_mean (should be positive!)
- pbrs_rewards/total_mean
- pbrs_rewards/pbrs_mean
- pbrs_summary/pbrs_contribution_ratio (should be 20-40%)
```

**Learning Metrics:**
```
- train/explained_variance (>0.7 is good)
- train/clip_fraction (<0.3 is stable)
- train/approx_kl (<0.05 is stable)
- loss/entropy (should stay high for exploration)
```

**Action Metrics:**
```
- actions/frequency/* (check NOOP < 10%)
- actions/entropy (>1.6 is good for 6 actions)
- actions/jump/frequency (30-40% is reasonable)
```

**Curriculum Metrics:**
```
- curriculum/current_stage_idx (should increase!)
- curriculum/episodes_in_stage
- curriculum/can_advance
```

### Alert Thresholds

**Critical Alerts (Stop Training):**
- ‚ùå NaN in loss
- ‚ùå Reward exploding (>1000)
- ‚ùå KL divergence >0.5 (policy collapse)
- ‚ùå Success rate stuck at 0% for >500K steps

**Warning Alerts (Investigate):**
- ‚ö†Ô∏è Average reward still negative after 1M steps
- ‚ö†Ô∏è Curriculum not advancing after 500K steps
- ‚ö†Ô∏è Explained variance <0.5
- ‚ö†Ô∏è NOOP >20%

**Good Signs (Keep Going):**
- ‚úÖ Average reward positive and increasing
- ‚úÖ Success rate increasing
- ‚úÖ Curriculum advancing
- ‚úÖ Explained variance >0.8

---

## Appendix C: Detailed Curriculum Analysis

### Stage Progression Timeline

```
Episodes by stage:
simplest: 129 (31%)
simpler:  191 (46%) ‚Üê Most common
simple:    94 (23%)
medium:     0 (0%)  ‚Üê Never reached
complex:    0 (0%)
exploration: 0 (0%)
mine_heavy:  0 (0%)

Time spent by stage:
Stages 0-2: 100% of training
Stages 3-6: 0% of training ‚Üê BIG PROBLEM
```

### Why Stuck?

**Threshold Analysis:**
```
Curriculum threshold: 50%
Simple stage success: 26.6%
Gap: 23.4 percentage points

Agent needs to improve success by 88% to advance!
This is too hard with current reward structure.
```

**Recommendation:**
```python
# Option 1: Lower threshold
threshold = 0.35  # More achievable

# Option 2: Adaptive threshold
if episodes_in_stage > 100:
    threshold = max(0.3, base_threshold - 0.1)

# Option 3: Trend-based advancement
if success_trend_positive and episodes > 50:
    can_advance = True
```

### Level Generator Success Heatmap

```
Generator Type         | Success Rate | Episodes | Avg Frames | Priority
-----------------------|--------------|----------|------------|----------
horizontal_corridor:*  | 75.0%        | 92       | 1500       | ‚úÖ Good
vertical_corridor:*    | 62.9%        | 104      | 1200       | ‚úÖ Good
corridors:simplest     | 69.0%        | 42       | 1800       | ‚úÖ Good
single_chamber:*       | 50.0%        | 32       | 2200       | ‚ö†Ô∏è Medium
corridors:simple       | 11.7%        | 60       | 4500       | ‚ùå Critical
maze:tiny              | 8.3%         | 12       | 4200       | ‚ùå Critical
jump_required:*        | 9.1%         | 11       | 2100       | ‚ùå Critical
vertical_corridor:plat | 9.1%         | 11       | 4300       | ‚ùå Critical
```

**Insights:**
1. **Corridors work well** - Agent learns linear navigation
2. **Mazes fail** - No sense of global navigation
3. **Jumps fail** - Precise timing not learned
4. **Platforms fail** - Vertical navigation weak

**Fix Priority:**
1. Add dense rewards for maze navigation
2. Add rewards for successful jumps
3. Reduce timeout on corridor levels (too easy)
4. Add curriculum sub-stages (corridors ‚Üí simple_jumps ‚Üí platforms ‚Üí mazes)

---

## Appendix D: References

### Papers & Resources

1. **Proximal Policy Optimization**
   - Schulman et al. (2017)
   - https://arxiv.org/abs/1707.06347

2. **Potential-Based Reward Shaping**
   - Ng et al. (1999)
   - https://people.eecs.berkeley.edu/~russell/papers/icml99-shaping.pdf

3. **Curriculum Learning**
   - Bengio et al. (2009)
   - https://qmro.qmul.ac.uk/xmlui/handle/123456789/15972

4. **Intrinsic Motivation (ICM)**
   - Pathak et al. (2017)
   - https://arxiv.org/abs/1705.05363

5. **Stable Baselines3 Documentation**
   - https://stable-baselines3.readthedocs.io/

6. **RL Tips & Tricks**
   - https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html

7. **Reward Engineering**
   - Freek Stulp's blog
   - http://www.freekstulp.net/

8. **Deep RL Doesn't Work Yet**
   - Alex Irpan (2018)
   - https://www.alexirpan.com/2018/02/14/rl-hard.html

### Code Examples

1. **RL Baselines3 Zoo**
   - Tuned hyperparameters for many environments
   - https://github.com/DLR-RM/rl-baselines3-zoo

2. **OpenAI Spinning Up**
   - Educational RL implementations
   - https://spinningup.openai.com/

3. **CleanRL**
   - Single-file RL implementations
   - https://github.com/vwxyzjn/cleanrl

---

**End of Analysis**

*This report represents a comprehensive analysis of your NPP-RL training run with actionable recommendations prioritized by impact. The most critical issue is the reward structure, which should be addressed immediately. With the recommended fixes, I expect significant improvements within 2-10M timesteps of training.*

*Good luck! This is a solvable problem, and you have a solid foundation to build on.*
